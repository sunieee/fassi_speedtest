# fassi测试

https://github.com/facebookresearch/faiss



### 使用多个标签

将单例改为多个标签一起inference

测试用例：28组promt，共703个词

|          | 单个标签 | 多个标签一起 |
| -------- | -------- | ------------ |
| Default  | 4.51746  | 3.82024      |
| Flat     | 5.48922  | 3.71875      |
| IVFxFlat | 3.8263   | 3.21174      |
| QPx      | 7.25442  | 3.5519       |
| IVFxPQy  | 3.18878  | 3.23568      |
| LSH      | 7.3807   | 3.6455       |
| HNSWx    | -        | -            |

HNSWx报错而无法运行，报错为：

```
==================== HNSWx ====================
train time: 0.0
Faiss assertion 'p + size == head_' failed in void faiss::gpu::StackDeviceMemory::Stack::returnAlloc(char*, size_t, cudaStream_t) at /root/miniconda3/conda-bld/faiss-pkg_1669821803039/work/faiss/gpu/utils/StackDeviceMemory.cpp:144
Aborted (core dumped)
```

Default加速比 = 4.51746 / 3.82024 = 1.18

### 统计

阈值为0.05

|          | inference time | train time | add time | acc     | #modify | #right |
| -------- | -------------- | ---------- | -------- | ------- | ------- | ------ |
| default  | 3.86058        | 0          | 0        | 1       | 703     | 703    |
| Flat     | 3.8599         | 0          | 1.01498  | 0.48649 | 345     | 342    |
| IVFxFlat | 3.32033        | 1.04713    | 1.15526  | 0       | 0       | 0      |
| PQx      | 3.65089        | 8.40904    | 3.05794  | 0.62731 | 703     | 441    |
| IVFxPQy  | 3.30469        | 4.54668    | 1.24999  | 0.71408 | 703     | 502    |
| LSH      | 3.70182        | 0          | 0.61448  | 0.52063 | 477     | 366    |

阈值为0

|          | inference time | train time | add time | acc     | #modify | #right |
| -------- | -------------- | ---------- | -------- | ------- | ------- | ------ |
| default  | 3.86128        | 0          | 0        | 1       | 703     | 703    |
| Flat     | 3.80253        | 0   | 1.00874  | 0.96017 | 678     | 675    |
| IVFxFlat | 3.28365        | 0.99777    | 1.14851  | 0       | 0       | 0      |
| PQx      | 3.61544        | 8.35615    | 2.98433  | 0.62731 | 703     | 441    |
| IVFxPQy  | 3.26414        | 4.35283    | 1.22339  | 0.70413 | 695     | 495    |
| LSH      | 3.7397         | 0          | 0.60988  | 0.52063 | 477     | 366    |


\#modify表示703个中修改过（匹配上）的单词，#right表示正确的单词

- 匹配数量（#modify）：发现使用default模型703个此都能匹配上，所以阈值thr并不重要。但是后面即使阈值取得非常低，部分方法#modify （修改的数量）也不多
- 准确率（acc=#right/703）：应该是越快acc越低，速度越慢(接近default)，acc就越高，但是这里都不太高。只有Flat再阈值为0时相当。
- 运行时间（inference time）：貌似并没有比default快多少呢，最快的方法（IVFxPQy）加速比为 3.86/3.32=1.16，甚至低于使用多个标签的加速比


### 添加HNSW

统一设置hnsw的表格超参如下：

```javascript
M = 32 
efSearch = 100  # number of entry points (neighbors) we use on each layer
efConstruction = 100 # number of entry points used on each layer during construction
```

整体上来说，要想要获得更快的构建和检索的速度，那么就需要把这三个超参相对地缩小，反之，要获得更好的召回精度，则需要将这三个超参增大。

- 设置了M=[16,32,64]，[efSearch, efConstruction]=[50,100]共12组超参数进行对比
- 对时间统计进行更精确化的统计：将搜索时间进行单独统计（inference为调用replace整个过程，而search指仅在词库里面搜索input_features过程）

表格中对比了6个普通方法和12个hnsw方法：

|                | inference time | train time | add time | search time | accuracy | #modify | #right |
| -------------- | -------------- | ---------- | -------- | ----------- | -------- | ------- | ------ |
| default        | 3.7985         | 0          | 0        | 0.0561      | 1        | 703     | 703    |
| Flat           | 3.8094         | 0          | 1.0153   | 0.5826      | 0.9602   | 678     | 675    |
| IVFxFlat       | 3.2608         | 1.0015     | 1.1525   | 0.027       | 0        | 0       | 0      |
| PQx            | 3.6788         | 7.3862     | 3.0438   | 0.3254      | 0.6273   | 703     | 441    |
| IVFxPQy        | 3.4599         | 4.8089     | 1.2423   | 0.0387      | 0.7084   | 699     | 498    |
| LSH            | 3.8911         | 0          | 0.6109   | 0.5319      | 0.5206   | 477     | 366    |
| HNSW16-50-50   | 3.3551         | 0          | 46.7039  | 0.1049      | 0.8521   | 703     | 599    |
| HNSW16-50-100  | 3.4102         | 0          | 84.9599  | 0.1001      | 0.862    | 703     | 606    |
| HNSW16-100-50  | 3.4670         | 0          | 45.3167  | 0.175       | 0.9075   | 703     | 638    |
| HNSW16-100-100 | 3.5798         | 0          | 87.2412  | 0.1569      | 0.936    | 703     | 658    |
| HNSW32-50-50   | 3.5378         | 0          | 128.206  | 0.177       | 0.9787   | 703     | 688    |
| HNSW32-50-100  | 3.4305         | 0          | 102.0471 | 0.1065      | 0.9104   | 703     | 640    |
| HNSW32-100-50  | 3.7273         | 0          | 131.0433 | 0.2728      | 0.9772   | 703     | 687    |
| HNSW32-100-100 | 3.4759         | 0          | 106.9338 | 0.1762      | 0.926    | 703     | 651    |
| HNSW64-50-50   | 3.5735         | 0          | 149.8472 | 0.2245      | 0.9701   | 703     | 682    |
| HNSW64-50-100  | 3.5544         | 0          | 269.9721 | 0.2488      | 0.9872   | 703     | 694    |
| HNSW64-100-50  | 3.8660         | 0          | 153.0835 | 0.4349      | 0.9673   | 703     | 680    |
| HNSW64-100-100 | 3.7379         | 0          | 274.2117 | 0.3714      | 0.9886   | 703     | 695    |

发现速度瓶颈并不在于匹配算法（矩阵乘法等），inference大部分在做tag的嵌入，真正做匹配的过程却耗时不长；如果需要使用fassi，还需要将输入改为float32，造成不必要的代价。

所以改变匹配这一部分提升性能意义不大。


### embed time时间统计

embed指tag嵌入部分的耗时：

```
    for tag in tags:
        tokenized_input = clip.tokenize(tag).to(device)
        input_features.append(model.encode_text(tokenized_input))
```

|                | inference time | train time | add time | search time | embed time | accuracy | #modify | #right |
| -------------- | -------------- | ---------- | -------- | ----------- | ---------- | -------- | ------- | ------ |
| default        | 3.6219         | 0          | 0        | 0.215       | 3.4039     | 1        | 703     | 703    |
| Flat           | 3.5261         | 0          | 1.0008   | 0.7013      | 2.8215     | 0.9602   | 678     | 675    |
| IVFxFlat       | 3.0617         | 0.9744     | 1.1342   | 0.2145      | 2.844      | 0        | 0       | 0      |
| PQx            | 3.3178         | 6.7626     | 2.8233   | 0.4605      | 2.854      | 0.6273   | 703     | 441    |
| IVFxPQy        | 3.1846         | 4.2859     | 1.215    | 0.2472      | 2.9334     | 0.7027   | 694     | 494    |
| LSH            | 3.5628         | 0          | 0.7695   | 0.63        | 2.9289     | 0.5206   | 477     | 366    |
| HNSW16-50-50   | 3.5681         | 0          | 47.6024  | 0.3579      | 3.2058     | 0.8307   | 703     | 584    |
| HNSW16-50-100  | 3.3234         | 0          | 91.3582  | 0.3343      | 2.9851     | 0.9047   | 703     | 636    |
| HNSW16-100-50  | 6.8052         | 0          | 45.4044  | 0.4439      | 6.3569     | 0.9189   | 703     | 646    |
| HNSW16-100-100 | 3.401          | 0          | 85.1919  | 0.3462      | 3.0512     | 0.9374   | 703     | 659    |
| HNSW32-50-50   | 3.311          | 0          | 125.0924 | 0.351       | 2.957      | 0.973    | 703     | 684    |
| HNSW32-50-100  | 3.2121         | 0          | 100.4916 | 0.3024      | 2.9064     | 0.9075   | 703     | 638    |
| HNSW32-100-50  | 3.5603         | 0          | 124.5644 | 0.4659      | 3.0907     | 0.9787   | 703     | 688    |
| HNSW32-100-100 | 3.2893         | 0          | 106.631  | 0.353       | 2.9328     | 0.9516   | 703     | 669    |
| HNSW64-50-50   | 3.3532         | 0          | 150.4489 | 0.3977      | 2.952      | 0.9502   | 703     | 668    |
| HNSW64-50-100  | 3.3273         | 0          | 269.3766 | 0.4155      | 2.9083     | 0.9858   | 703     | 693    |
| HNSW64-100-50  | 3.7395         | 0          | 146.1938 | 0.6304      | 3.1048     | 0.9516   | 703     | 669    |
| HNSW64-100-100 | 3.5248         | 0          | 256.9456 | 0.5664      | 2.9553     | 0.9886   | 703     | 695    |


### clip并行

> `model.encode_text(text: Tensor)`
>
> Given a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.

按照官方提供的样例，不再对每个标签embed一次，而是将embed过程改为并行：

```python
tokenized_input = torch.cat([clip.tokenize(tag) for tag in tags]).to(device)
input_features = model.encode_text(tokenized_input)
```

|                | inference time | train time | add time | search time | embed time | accuracy | #modify | #right |
| -------------- | -------------- | ---------- | -------- | ----------- | ---------- | -------- | ------- | ------ |
| default        | 0.7059         | 0          | 0        | 0.0592      | 0.6454     | 1        | 703     | 703    |
| Flat           | 0.9183         | 0          | 1.0005   | 0.5119      | 0.4049     | 0.963    | 680     | 677    |
| IVFxFlat       | 0.4462         | 0.9756     | 1.1332   | 0.0323      | 0.4127     | 0.027    | 58      | 19     |
| PQx            | 0.6647         | 7.791      | 2.8452   | 0.2379      | 0.4248     | 0.6216   | 703     | 437    |
| IVFxPQy        | 0.4401         | 4.0378     | 1.2072   | 0.0352      | 0.4032     | 0.7027   | 699     | 494    |
| LSH            | 0.7798         | 0          | 0.5828   | 0.3603      | 0.4174     | 0.5036   | 469     | 354    |
| HNSW16-100-100 | 0.5981         | 0          | 82.7932  | 0.1458      | 0.45       | 0.9317   | 703     | 655    |
| HNSW32-100-100 | 0.5859         | 0          | 106.9764 | 0.1319      | 0.4521     | 0.9531   | 703     | 670    |
| HNSW64-100-100 | 0.7905         | 0          | 258.4763 | 0.3461      | 0.4423     | 0.9872   | 703     | 694    |

embed time加速比 = 3.4039 / 0.6454 = 5.27

inference time加速比 = 3.6219 / 0.7059 = 5.13